# -*- coding: utf-8 -*-
"""
Created on Tue Dec 25 18:51:51 2018

@author: robin
"""

from functions import *
import pandas as pd
from statsmodels.graphics.tsaplots import plot_pacf
from statsmodels.graphics.tsaplots import plot_acf
import numpy.random as rnd
import numpy as np
import matplotlib.pyplot as plt
#==============================================================================
# Simulation of a dataset
#==============================================================================

### Linear model

# Residuals
mu = 0
sigma = 1
n = 1000
epsilon = rnd.normal(mu, sigma, n)

#plt.hist(epsilon, bins=100)

# Covariates
mean = [5, -20, 3, 4]
cov = [[10, 0, 0, 0],
       [0, 100, 0, 0],
       [0, 0, 10, 0],
       [0, 0, 0, 5]]  # diagonal covariance
X = rnd.multivariate_normal(mean, cov, n)


# Model y = 2 + beta*x + epsilon
beta = [1, 2, 3, -2]
b0 = 0
y = b0 + np.dot(X, beta) + epsilon
Y = np.reshape(y, (-1,1))


### Model 1
def simul_model1(n, with_cst=False ,seed=None): # Model 1 of Kocherginsky (2002)
    """ Simulate y= 1 + b1*x1 + b2*x2 + b3*x3 + e, with x1 following a standard normal, 
    x3 is a U[0,1], x2=x1 + x3 + z, with z and e following a standard normal"""
    rnd = np.random.RandomState(seed)
    x1,e,z = rnd.normal(size=n).reshape((-1,1)), rnd.normal(size=n).reshape((-1,1)), rnd.normal(size=n).reshape((-1,1)) # generate 3 n-sample of std normal 
    x3 = rnd.rand(n).reshape((-1,1))
    x2 = x1+x3+z
    X = np.hstack([np.ones((n,1)), x1, x2, x3]) if with_cst else np.hstack([x1, x2, x3])
    coefs_ = np.ones((X.shape[1],1)).reshape(-1,1)
    return (np.dot(X,coefs_) + e,X)


def simul_indep_multi_gaussian(size, sigma, mean ,seed=None):
    """ Simulate a multivariate gaussian matrix of size "size" 
    independant gaussian vector of variance sigma (and null covariance by construction)"""
    mean = (1, 2)
    cov = [[1, 0], [0, 1]]
    rnd = np.random.RandomState(seed)
    x = rnd.multivariate_normal(mean, cov, (3, 3))
    return x

#==============================================================================
# Test
#==============================================================================
Kn = 100
seed = 2042

model1 = simul_model1(n, seed=seed)
Y = model1[0]
X = model1[1]
tau = 0.5

p = X.shape[1]

# MCMB
beta, IC = MCMB(Y=Y, X=X, tau=tau, size=Kn,seed=seed, alpha=0.05, parallelize_mode='p')
IC

#==============================================================================
# Burn-in and autocorrelation computation and plot
#==============================================================================

# Burn-in evaluation
plot_same_graph(betas_chains, autocorr=False, title=' - classic method') # 10-18 iterations
plot_same_graph(betas_chains_A, autocorr=False, title=' - A method') # 3 iterations
plot_same_graph(betas_chains_7s, autocorr=False, title=' - 7 sample spacing method') # 1-3 iterations
plot_same_graph(betas_chains_p, autocorr=False, title=' - parallelized method') # 10-15 iterations with more variations
plot_same_graph(betas_chains_A_p, autocorr=False, title=' - A and parallelized method') # 3 iterations


####### Autocorrelations computation
p = X.shape[1]
# For the classic MCMB
chain = MCMB(Y=Y, X=X, tau=tau, size=Kn,seed=seed, alpha=0.05, return_chain=True)
betas_chains = [pd.Series([chain[i][j] for i in range(Kn)]) for j in range(p)] # Get the p betas chains generated by the MCMB algorithm 

# For the  MCMB-A
chain_A = MCMB(Y=Y, X=X, tau=tau, size=Kn,seed=seed, alpha=0.05, return_chain=True, extension='A')
betas_chains_A = [pd.Series([chain_A[i][j] for i in range(Kn)]) for j in range(p)] # Get the p betas chains generated by the MCMB algorithm 

# For the classic MCMB with the sample-spacing = 7
chain_7s = MCMB(Y=Y, X=X, tau=tau, size=Kn,seed=seed, alpha=0.05, return_chain=True, sample_spacing=7)
betas_chains_7s = [pd.Series([chain_7s[i][j] for i in range(Kn)]) for j in range(p)] # Get the p betas chains generated by the MCMB algorithm 
# kill the autocorrelations

# For the parallelized version of MCMB
chain_p = MCMB(Y=Y, X=X, tau=tau, size=Kn,seed=seed, alpha=0.05, return_chain=True, parallelize_mode='p')
betas_chains_p = [pd.Series([chain_p[i][j] for i in range(Kn)]) for j in range(p)] # Get the p betas chains generated by the MCMB algorithm 
# The betas are more correlated than in the MCMB classic version

# For the parallelized version of MCMB-A
chain_A_p = MCMB(Y=Y, X=X, tau=tau, size=Kn,seed=seed, alpha=0.05, return_chain=True, extension='A',parallelize_mode='p')
betas_chains_A_p = [pd.Series([chain_A_p[i][j] for i in range(Kn)]) for j in range(p)] # Get the p betas chains generated by the MCMB algorithm 

## Autocorrelations plot

plot_same_graph(betas_chains, title=' - classic method') # Very very correlated
plot_same_graph(betas_chains_A, title=' - A method') # Less correlated: From 21 X corr
plot_same_graph(betas_chains_7s, title=' - 7 sample spacing method') # No more correlation from 20 iters
plot_same_graph(betas_chains_p, title=' - parallelized method') # Very weird pattern: stationnary oscillations: -1/1
plot_same_graph(betas_chains_A_p, title=' - A and parallelized method') # Same but a little less attenuated




